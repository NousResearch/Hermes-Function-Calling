{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Connected Function Calling with llama.cpp\n",
    "### Adapted from the Ollama Notebook\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### 1. Install llama.cpp\n",
    "llama.cpp installation instructions per OS (macOS, Linux, Windows) can be found on [their website](https://llama-cpp-python.readthedocs.io/en/latest/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Python llama.cpp Library\n",
    "\n",
    "For that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Pull the model from HuggingFace\n",
    "\n",
    "Download the GGUF NousHermes-2-Pro-Mistral-7B from HuggingFace (uploaded by adrienbrault) [here](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF). :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "#### 1. Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_weather_forecast(location: str) -> dict[str, str]:\n",
    "    \"\"\"Retrieves the weather forecast for a given location\"\"\"\n",
    "    # Mock values for test\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"forecast\": \"sunny\",\n",
    "        \"temperature\": \"25°C\",\n",
    "    }\n",
    "\n",
    "def get_random_city() -> str:\n",
    "    \"\"\"Retrieves a random city from a list of cities\"\"\"\n",
    "    cities = [\"Groningen\", \"Enschede\", \"Amsterdam\", \"Istanbul\", \"Baghdad\", \"Rio de Janeiro\", \"Tokyo\", \"Kampala\"]\n",
    "    return random.choice(cities)\n",
    "\n",
    "def get_random_number() -> int:\n",
    "    \"\"\"Retrieves a random number\"\"\"\n",
    "    # Mock value for test\n",
    "    return 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Function Caller\n",
    "\n",
    "For this example in Jupyter format, I'm simply putting the functions in a list. In a python project, you can use the implementation here as an inspiration: https://github.com/AtakanTekparmak/ollama_langhcain_fn_calling/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class FunctionCaller:\n",
    "    \"\"\"\n",
    "    A class to call functions from tools.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize the functions dictionary\n",
    "        self.functions = {\n",
    "            \"get_weather_forecast\": get_weather_forecast,\n",
    "            \"get_random_city\": get_random_city,\n",
    "            \"get_random_number\": get_random_number,\n",
    "        }\n",
    "        self.outputs = {}\n",
    "\n",
    "    def create_functions_metadata(self) -> list[dict]:\n",
    "        \"\"\"Creates the functions metadata for the prompt. \"\"\"\n",
    "        def format_type(p_type: str) -> str:\n",
    "            \"\"\"Format the type of the parameter.\"\"\"\n",
    "            # If p_type begins with \"<class\", then it is a class type\n",
    "            if p_type.startswith(\"<class\"):\n",
    "                # Get the class name from the type\n",
    "                p_type = p_type.split(\"'\")[1]\n",
    "            \n",
    "            return p_type\n",
    "            \n",
    "        functions_metadata = []\n",
    "        i = 0\n",
    "        for name, function in self.functions.items():\n",
    "            i += 1\n",
    "            descriptions = function.__doc__.split(\"\\n\")\n",
    "            print(descriptions)\n",
    "            functions_metadata.append({\n",
    "                \"name\": name,\n",
    "                \"description\": descriptions[0],\n",
    "                \"parameters\": {\n",
    "                    \"properties\": [ # Get the parameters for the function\n",
    "                        {   \n",
    "                            \"name\": param_name,\n",
    "                            \"type\": format_type(str(param_type)),\n",
    "                        }\n",
    "                        # Remove the return type from the parameters\n",
    "                        for param_name, param_type in function.__annotations__.items() if param_name != \"return\"\n",
    "                    ],\n",
    "                    \n",
    "                    \"required\": [param_name for param_name in function.__annotations__ if param_name != \"return\"],\n",
    "                } if function.__annotations__ else {},\n",
    "                \"returns\": [\n",
    "                    {\n",
    "                        \"name\": name + \"_output\",\n",
    "                        \"type\": {param_name: format_type(str(param_type)) for param_name, param_type in function.__annotations__.items() if param_name == \"return\"}[\"return\"]\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "\n",
    "        return functions_metadata\n",
    "\n",
    "    def call_function(self, function):\n",
    "        \"\"\"\n",
    "        Call the function from the given input.\n",
    "\n",
    "        Args:\n",
    "            function (dict): A dictionary containing the function details.\n",
    "        \"\"\"\n",
    "    \n",
    "        def check_if_input_is_output(input: dict) -> dict:\n",
    "            \"\"\"Check if the input is an output from a previous function.\"\"\"\n",
    "            for key, value in input.items():\n",
    "                if value in self.outputs:\n",
    "                    input[key] = self.outputs[value]\n",
    "            return input\n",
    "\n",
    "        # Get the function name from the function dictionary\n",
    "        function_name = function[\"name\"]\n",
    "        \n",
    "        # Get the function params from the function dictionary\n",
    "        function_input = function[\"params\"] if \"params\" in function else None\n",
    "        function_input = check_if_input_is_output(function_input) if function_input else None\n",
    "    \n",
    "        # Call the function from tools.py with the given input\n",
    "        # pass all the arguments to the function from the function_input\n",
    "        output = self.functions[function_name](**function_input) if function_input else self.functions[function_name]()\n",
    "        self.outputs[function[\"output\"]] = output\n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Setup The Function Caller and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retrieves the weather forecast for a given location']\n",
      "['Retrieves a random city from a list of cities']\n",
      "['Retrieves a random number']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the FunctionCaller \n",
    "function_caller = FunctionCaller()\n",
    "\n",
    "# Create the functions metadata\n",
    "functions_metadata = function_caller.create_functions_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create the system prompt\n",
    "prompt_beginning = \"\"\"\n",
    "You are an AI assistant that can help the user with a variety of tasks. You have access to the following functions:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_end = \"\"\"\n",
    "\n",
    "When the user asks you a question, if you need to use functions, provide ONLY the function calls, and NOTHING ELSE, in the format:\n",
    "<function_calls>    \n",
    "[\n",
    "    { \"name\": \"function_name_1\", \"params\": { \"param_1\": \"value_1\", \"param_2\": \"value_2\" }, \"output\": \"The output variable name, to be possibly used as input for another function},\n",
    "    { \"name\": \"function_name_2\", \"params\": { \"param_3\": \"value_3\", \"param_4\": \"output_1\"}, \"output\": \"The output variable name, to be possibly used as input for another function\"},\n",
    "    ...\n",
    "]\n",
    "\"\"\"\n",
    "system_prompt = prompt_beginning + f\"<tools> {json.dumps(functions_metadata, indent=4)} </tools>\" + system_prompt_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Hermes-2-Pro-Llama-3-Instruct-Merged-DPO\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128003\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 14.96 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Hermes-2-Pro-Llama-3-Instruct-Merged-DPO\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128003 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128003 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size = 14315.02 MiB, (14315.08 / 49152.00)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 15317.02 MiB\n",
      "llm_load_tensors:      Metal buffer size = 14315.00 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 1\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Max\n",
      "ggml_metal_init: picking default device: Apple M1 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =    88.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 903\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128003', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '1', 'llama.feed_forward_length': '14336', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Hermes-2-Pro-Llama-3-Instruct-Merged-DPO', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "model = llama_cpp.Llama(\n",
    "    model_path='Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-F16.gguf',\n",
    "    n_gpu_layers=32,\n",
    "    n_threads=10,\n",
    "    use_mlock=True,\n",
    "    flash_attn=True,\n",
    "    n_ctx=8192,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1707.16 ms\n",
      "llama_print_timings:      sample time =      20.74 ms /    63 runs   (    0.33 ms per token,  3037.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1706.23 ms /   450 tokens (    3.79 ms per token,   263.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4579.55 ms /    62 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =    6663.81 ms /   512 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8e53b561-32d6-433b-802d-5bb982660363', 'object': 'chat.completion', 'created': 1717239441, 'model': 'Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-F16.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '[\\n    {\\n        \"name\": \"get_random_city\",\\n        \"params\": {},\\n        \"output\": \"random_city\"\\n    },\\n    {\\n        \"name\": \"get_weather_forecast\",\\n        \"params\": {\"location\": \"random_city\"},\\n        \"output\": \"weather_forecast\"\\n    }\\n]'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 450, 'completion_tokens': 62, 'total_tokens': 512}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compose the prompt \n",
    "user_query = \"Whats the temperature in a random city?\"\n",
    "\n",
    "# Get the response from the model\n",
    "model_name = 'adrienbrault/nous-hermes2pro:Q8_0'\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_prompt,\n",
    "    },\n",
    "    {'role': 'user', 'content': user_query}\n",
    "]\n",
    "response = model.create_chat_completion(messages=messages)\n",
    "print(response)\n",
    "# Get the function calls from the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n    {\\n        \"name\": \"get_random_city\",\\n        \"params\": {},\\n        \"output\": \"random_city\"\\n    },\\n    {\\n        \"name\": \"get_weather_forecast\",\\n        \"params\": {\"location\": \"random_city\"},\\n        \"output\": \"weather_forecast\"\\n    }\\n]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calls:\n",
      "[{'name': 'get_random_city', 'params': {}, 'output': 'random_city'}, {'name': 'get_weather_forecast', 'params': {'location': 'random_city'}, 'output': 'weather_forecast'}]\n"
     ]
    }
   ],
   "source": [
    "function_calls = response['choices'][0]['message']['content']\n",
    "# If it ends with a <function_calls>, get everything before it\n",
    "if function_calls.startswith(\"<function_calls>\"):\n",
    "    function_calls = function_calls.split(\"<function_calls>\")[1]\n",
    "\n",
    "# Read function calls as json\n",
    "try:\n",
    "    function_calls_json: list[dict[str, str]] = json.loads(function_calls)\n",
    "except json.JSONDecodeError:\n",
    "    function_calls_json = []\n",
    "    print (\"Model response not in desired JSON format\")\n",
    "finally:\n",
    "    print(\"Function calls:\")\n",
    "    print(function_calls_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append the assistant message to the chat and call the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <tool_call> to the function calls as mentioned in the chat template in Hugging Face\n",
    "function_message = '<tool_call>' + str(function_calls_json) + '</tool_call>'\n",
    "\n",
    "messages.append({'role': 'assistant', 'content': function_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Response: Amsterdam\n",
      "Tool Response: {'location': 'Groningen', 'forecast': 'sunny', 'temperature': '25°C'}\n"
     ]
    }
   ],
   "source": [
    "for function in function_calls_json:\n",
    "    output = f\"Tool Response: {function_caller.call_function(function)}\"\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the functions\n",
    "output = \"\"\n",
    "for function in function_calls_json:\n",
    "    output = f\"{function_caller.call_function(function)}\"\n",
    "\n",
    "#Append the tool response to the messages with the chat format\n",
    "tool_output = '<tool_response> ' + output + ' </tool_response>'\n",
    "messages.append({'role': 'tool', 'content': tool_output})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\nYou are an AI assistant that can help the user with a variety of tasks. You have access to the following functions:\\n\\n<tools> [\\n    {\\n        \"name\": \"get_weather_forecast\",\\n        \"description\": \"Retrieves the weather forecast for a given location\",\\n        \"parameters\": {\\n            \"properties\": [\\n                {\\n                    \"name\": \"location\",\\n                    \"type\": \"str\"\\n                }\\n            ],\\n            \"required\": [\\n                \"location\"\\n            ]\\n        },\\n        \"returns\": [\\n            {\\n                \"name\": \"get_weather_forecast_output\",\\n                \"type\": \"dict[str, str]\"\\n            }\\n        ]\\n    },\\n    {\\n        \"name\": \"get_random_city\",\\n        \"description\": \"Retrieves a random city from a list of cities\",\\n        \"parameters\": {\\n            \"properties\": [],\\n            \"required\": []\\n        },\\n        \"returns\": [\\n            {\\n                \"name\": \"get_random_city_output\",\\n                \"type\": \"str\"\\n            }\\n        ]\\n    },\\n    {\\n        \"name\": \"get_random_number\",\\n        \"description\": \"Retrieves a random number\",\\n        \"parameters\": {\\n            \"properties\": [],\\n            \"required\": []\\n        },\\n        \"returns\": [\\n            {\\n                \"name\": \"get_random_number_output\",\\n                \"type\": \"int\"\\n            }\\n        ]\\n    }\\n] </tools>\\n\\nWhen the user asks you a question, if you need to use functions, provide ONLY the function calls, and NOTHING ELSE, in the format:\\n<function_calls>    \\n[\\n    { \"name\": \"function_name_1\", \"params\": { \"param_1\": \"value_1\", \"param_2\": \"value_2\" }, \"output\": \"The output variable name, to be possibly used as input for another function},\\n    { \"name\": \"function_name_2\", \"params\": { \"param_3\": \"value_3\", \"param_4\": \"output_1\"}, \"output\": \"The output variable name, to be possibly used as input for another function\"},\\n    ...\\n]\\n'},\n",
       " {'role': 'user', 'content': 'Whats the temperature in a random city?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<tool_call>[{'name': 'get_random_city', 'params': {}, 'output': 'random_city'}, {'name': 'get_weather_forecast', 'params': {'location': 'Groningen'}, 'output': 'weather_forecast'}]</tool_call>\"},\n",
       " {'role': 'tool',\n",
       "  'content': \"<tool_response> {'location': 'Groningen', 'forecast': 'sunny', 'temperature': '25°C'} </tool_response>\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference the model again with the tool respones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1707.16 ms\n",
      "llama_print_timings:      sample time =      14.45 ms /    20 runs   (    0.72 ms per token,  1384.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     775.81 ms /    86 tokens (    9.02 ms per token,   110.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1416.85 ms /    19 runs   (   74.57 ms per token,    13.41 tokens per second)\n",
      "llama_print_timings:       total time =    2321.63 ms /   105 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-25dfeae2-2184-497f-b838-e08565ad078c',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1717239671,\n",
       " 'model': 'Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-F16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"The temperature in the random city of Groningen is currently 25°C and it's sunny.\"},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 536, 'completion_tokens': 19, 'total_tokens': 555}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model.create_chat_completion(messages=messages,temperature=0)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
